Overfit a tiny model on a small dataset to ensure the training loop works.
Takes about 20 minutes on an A100 GPU.

```
"--image_encoder_model_name_or_path", "WinKawaks/vit-tiny-patch16-224",
"--bytes_encoder_model_name_or_path", "prajjwal1/bert-tiny",
"--latent_transformer_model_name_or_path", "sbintuitions/tiny-lm",
"--bytes_decoder_model_name_or_path", "sbintuitions/tiny-lm",
"--dataset_name", "wikitext",
"--dataset_config_name", "wikitext-2-raw-v1",
"--remove_unused_columns", "False",
"--per_device_train_batch_size", "8",
"--per_device_eval_batch_size", "8",
"--auto_find_batch_size", "true",
"--do_train", "True",
# "--do_eval", "True",
"--output_dir", MODEL_OUTPUT_DIR,
# "--overwrite_output_dir", "true",
"--logging_steps", "10",
"--logging_strategy", "steps",
"--max_steps", "3000",
"--warmup_freeze_steps", "0",
"--max_sequence_length", "32",
"--max_word_length", "16",
"--dataloader_num_workers", "4",
"--dataloader_pin_memory", "True",
"--include_tokens_per_second", "True",
"--include_num_input_tokens_seen", "True",
"--learning_rate", "3e-4",
"--torch_dtype", "bfloat16",
"--bf16", "True",
"--report_to", "wandb",
"--max_train_samples", "128",
```